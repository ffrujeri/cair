{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "820f08c4-6816-4621-bc32-d027f9f32a37",
   "metadata": {},
   "source": [
    "## Denominator Layout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "45d373a6-c6a8-4b76-b78e-1fd7f627ecc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class MLPLayer:\n",
    "    def __init__(self, input_size, output_size, activation='sigmoid'):\n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "        self.activation = activation\n",
    "\n",
    "        # Initialize weights and biases\n",
    "        self.weights = np.random.randn(output_size, input_size)\n",
    "        print('w', self.weights.shape)\n",
    "\n",
    "        self.biases = np.zeros((output_size, 1))\n",
    "        print('b', self.biases.shape)\n",
    "        \n",
    "    def forward(self, X):\n",
    "        self.X = X\n",
    "        print('X', self.X.shape)\n",
    "        \n",
    "        self.z = np.dot(self.weights, X) + self.biases\n",
    "        print('z', self.z.shape)\n",
    "        \n",
    "        self.a = self._activate(self.z)\n",
    "        print('a', self.a.shape)\n",
    "\n",
    "        return self.a\n",
    "\n",
    "    def backward(self, dL_da, learning_rate):\n",
    "        print('dL/da', dL_da.shape)\n",
    "        \n",
    "        da_dz = self._activate_derivative(self.z)\n",
    "        print('da/dz', da_dz.shape)\n",
    "\n",
    "        dL_dz = da_dz * dL_da\n",
    "        print('dL/dz', dL_dz.shape)\n",
    "\n",
    "        dz_dw = self.X.T\n",
    "        dz_db = 1\n",
    "\n",
    "        dL_dw = np.dot(dL_dz, dz_dw)\n",
    "        print('dL/dW', dL_dw.shape)\n",
    "        \n",
    "        dL_db = dz_db * np.sum(dL_dz, axis=1, keepdims=True)\n",
    "        print('dL/db', dL_db.shape)\n",
    "\n",
    "        dL_da_prev = np.dot(self.weights.T, dL_dz)\n",
    "        print('dL/dX (l-1)', dL_da_prev.shape)\n",
    "\n",
    "        # Update weights and biases\n",
    "        self.weights -= learning_rate * dL_dw\n",
    "        self.biases -= learning_rate * dL_db\n",
    "\n",
    "        return dL_da_prev\n",
    "\n",
    "    def _activate(self, x):\n",
    "        if self.activation == 'sigmoid':\n",
    "            return 1 / (1 + np.exp(-x))\n",
    "        elif self.activation == 'relu':\n",
    "            return np.maximum(0, x)\n",
    "        elif self.activation == 'tanh':\n",
    "            return np.tanh(x)\n",
    "        else:\n",
    "            raise ValueError(\"Invalid activation function.\")\n",
    "\n",
    "    def _activate_derivative(self, x):\n",
    "        if self.activation == 'sigmoid':\n",
    "            sig = self._activate(x)\n",
    "            return sig * (1 - sig)\n",
    "        elif self.activation == 'relu':\n",
    "            return np.where(x > 0, 1, 0)\n",
    "        elif self.activation == 'tanh':\n",
    "            return 1 - np.tanh(x) ** 2\n",
    "        else:\n",
    "            raise ValueError(\"Invalid activation function.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f0f151e9-c947-4e39-91d1-3397b9752ae8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w (2, 3)\n",
      "b (2, 1)\n",
      "X (3, 4)\n",
      "z (2, 4)\n",
      "a (2, 4)\n",
      "Forward pass output: (2, 4)\n"
     ]
    }
   ],
   "source": [
    "# Create an MLP layer with input size 3 and output size 2\n",
    "input_size = 3\n",
    "output_size = 2\n",
    "\n",
    "mlp_layer = MLPLayer(input_size, output_size, activation='sigmoid')\n",
    "\n",
    "# Perform forward pass\n",
    "inputs = np.array([[0.2, 0.3, 0.4],\n",
    "                   [0.5, 0.6, 0.7],\n",
    "                   [0.8, 0.9, 1.0],\n",
    "                   [1.1, 1.2, 1.3]]).T\n",
    "\n",
    "output = mlp_layer.forward(inputs)\n",
    "print(\"Forward pass output:\", output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3ea5accf-3e7b-47d6-a073-74dbdebfbd58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dL/da (2, 4)\n",
      "da/dz (2, 4)\n",
      "dL/dz (2, 4)\n",
      "dL/dW (2, 3)\n",
      "dL/db (2, 1)\n",
      "dL/dX (l-1) (3, 4)\n",
      "Gradient w.r.t. input: [[-0.11674066 -0.07172005 -0.20094498 -0.09486496]\n",
      " [-0.01086257 -0.06146702  0.01070669 -0.01134419]\n",
      " [ 0.04315149  0.12159974  0.02324768  0.03943372]]\n"
     ]
    }
   ],
   "source": [
    "# Perform backward pass with gradient of loss w.r.t. output and learning rate\n",
    "dL_da = np.array([[0.1, 0.2],\n",
    "                  [0.3, 0.1],\n",
    "                  [0.05, 0.6],\n",
    "                  [0.1, 0.4]]).T\n",
    "learning_rate = 0.1\n",
    "dL_da_prev = mlp_layer.backward(dL_da, learning_rate)\n",
    "print(\"Gradient w.r.t. input:\", dL_da_prev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30e22439-2893-49f8-9bf7-2e3ff36e8f54",
   "metadata": {},
   "outputs": [],
   "source": [
    "A = np.random.randn(3,2)\n",
    "B = np.random.randn(3,2)\n",
    "np.dot(A,B)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de69e375-d490-4177-b987-b6602a998f5f",
   "metadata": {},
   "source": [
    "# Numerator layout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "id": "97687de3-ebaa-45b0-ba44-96d3b8062512",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class TorchMLP(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(TorchMLP, self).__init__()\n",
    "        \n",
    "        self.fc1 = nn.Linear(input_size, output_size)  # First fully connected layer\n",
    "        self.activation = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.fc1(x)  # Pass the input through the first fully connected layer\n",
    "        out = self.activation(out)  # Apply the ReLU activation function\n",
    "        self.out = out\n",
    "        self.out.retain_grad()\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "id": "ac6fe4c7-5ad4-476a-a76b-706af6a81ddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "t_mlp = TorchMLP(input_size, output_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "id": "4eb55ddc-6d40-4d31-bb13-f466e27202dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[ 0.2781,  0.4060,  0.4201],\n",
      "        [-0.4212, -0.1295,  0.3830]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-0.2347,  0.1003], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "print(t_mlp.fc1.weight)\n",
    "print(t_mlp.fc1.bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "id": "150c532a-77a0-47e7-adfe-8abed174768d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#L = 1/2n * sum((Y - A)^2)\n",
    "\n",
    "# dA = dL/dA = d(1/2 * sum((Y - A)^2))/dA\n",
    "#    = -2 * 1/2 * (Y - A)\n",
    "#    = A - Y\n",
    "\n",
    "# dZ = dL/dZ = dL/dA * dA/dZ\n",
    "#    = (A - Y) * sigmoid_derivative(Z)\n",
    "\n",
    "# dW = dL/dW = dL/dZ * dZ/dW\n",
    "#    = X.T * dZ\n",
    "\n",
    "# db = dL/db = dL/dZ * dZ/db\n",
    "#    = sum(dZ, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 492,
   "id": "8500c715-63da-41d8-9740-4aece2a12c9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class MLP:\n",
    "    def __init__(self, input_size, output_size):\n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "        self.weights = np.random.randn(input_size, output_size)\n",
    "        self.biases = np.zeros(output_size)\n",
    "    \n",
    "    def forward(self, X):\n",
    "        self.X = X\n",
    "        self.z = np.matmul(X, self.weights) + self.biases\n",
    "        self.a = self.sigmoid(self.z)\n",
    "        return self.a\n",
    "    \n",
    "    def backward(self, dA, learning_rate):\n",
    "        dZ = self.sigmoid_derivative(self.z) * dA        # dL/dZ = dA/dZ * dL/dA\n",
    "        dW = np.matmul(self.X.T, dZ)                     # dL/dW = dZ/dW * dL/dZ\n",
    "        db = np.sum(1 * dZ, axis=0)                      # dL/db = dZ/db * dL/dZ\n",
    "        dX = np.matmul(dZ, self.weights.T)               # dL/dX = dZ/dX * dL/dZ\n",
    "        \n",
    "        self.weights -= learning_rate * dW\n",
    "        self.biases -= learning_rate * db\n",
    "        \n",
    "        return dX\n",
    "    \n",
    "    def sigmoid(self, x):\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "    \n",
    "    def sigmoid_derivative(self, x):\n",
    "        return x * (1 - x)\n",
    "\n",
    "    def mse_loss(self, output, target):\n",
    "        return np.mean((output - target) ** 2)\n",
    "\n",
    "    def mse_loss_derivative(self, predicted, targets):\n",
    "        return predicted - targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 493,
   "id": "3a7bdd2f-dec5-4a5e-91f7-1a743f9d1262",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform forward pass\n",
    "X = np.random.randn(10, input_size)  # Example input\n",
    "Y = np.random.randn(10, output_size)  # Example input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 494,
   "id": "e9931a56-01d4-4250-a9c4-136d65df5c18",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 3\n",
    "output_size = 2\n",
    "\n",
    "mlp = MLP(input_size, output_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 495,
   "id": "759aedf4-1ae5-42a4-9b9c-365770bc7176",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.1972055 , 0.43293981],\n",
       "       [0.16776044, 0.54691291],\n",
       "       [0.25696774, 0.65363529],\n",
       "       [0.48832962, 0.24477543],\n",
       "       [0.05333731, 0.5989532 ],\n",
       "       [0.6626236 , 0.78894806],\n",
       "       [0.87031847, 0.17446047],\n",
       "       [0.73584926, 0.49145121],\n",
       "       [0.03747002, 0.48445427],\n",
       "       [0.94109756, 0.28112193]])"
      ]
     },
     "execution_count": 495,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = mlp.forward(X)\n",
    "A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 496,
   "id": "39e9c4a4-8ee8-4a23-82a8-f3f62f7fe378",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.11374033,  1.70354183],\n",
       "       [ 1.47290517,  0.44575059],\n",
       "       [-0.28471107, -0.32983539],\n",
       "       [ 0.67264599,  1.56807743],\n",
       "       [-0.00514339,  1.61655007],\n",
       "       [-0.43781747,  1.43233171],\n",
       "       [ 0.06125633, -0.36610595],\n",
       "       [-0.24974756, -0.21364742],\n",
       "       [-0.26749673,  0.65657288],\n",
       "       [ 1.32684866,  0.34042252]])"
      ]
     },
     "execution_count": 496,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss = mlp.mse_loss(A, Y)\n",
    "dA = mlp.mse_loss_derivative(A, Y)\n",
    "dA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 497,
   "id": "76724f8f-f4ad-42f0-ad4c-6579883142ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-5.67141215e-01, -9.39809417e-01, -5.11400675e-01],\n",
       "       [ 5.53874548e-01,  1.26617220e+01,  4.38169234e+00],\n",
       "       [-1.20134852e-01, -1.30367383e+00, -4.73481706e-01],\n",
       "       [-3.44965004e+00, -8.91144361e-01, -1.51548710e+00],\n",
       "       [ 3.52219682e-01, -1.90467960e-02,  1.18360732e-01],\n",
       "       [-5.45197210e-01,  4.43190500e-02, -1.78304690e-01],\n",
       "       [ 1.34401952e+00,  5.88109539e-01,  6.70088819e-01],\n",
       "       [ 6.44106402e-03, -1.09900415e-02, -1.35343434e-03],\n",
       "       [-3.35005650e-01, -7.60710371e+00, -2.63329331e+00],\n",
       "       [-4.80771580e-02,  1.32594982e+01,  4.36626101e+00]])"
      ]
     },
     "execution_count": 497,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learning_rate = 0.1\n",
    "dX = mlp.backward(dA, learning_rate)\n",
    "dX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 482,
   "id": "aab3e155-f604-48e4-8090-8c3abfe34dec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class MLP:\n",
    "    def __init__(self, input_size, output_size):\n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "        self.weights = np.random.randn(input_size, output_size)\n",
    "        self.biases = np.zeros((1, output_size))\n",
    "    \n",
    "    def forward(self, X):\n",
    "        self.X = X\n",
    "        self.z = np.matmul(X, self.weights) + self.biases\n",
    "        self.a = self.sigmoid(self.z)\n",
    "        return self.a\n",
    "    \n",
    "    def backward(self, targets, learning_rate):\n",
    "        dA = self.mse_loss_derivative(self.a, targets)\n",
    "        dZ = self.sigmoid_derivative(self.z) * dA        # dL/dz = dA/dz * dL/dA\n",
    "        dW = np.matmul(self.X.T, dZ)                     # dL/dW = dz/dW * dL/dz\n",
    "        db = np.matmul(np.ones((1, dZ.shape[0])), dZ)    # dL/db = dz/db * dL/dz\n",
    "        dX = np.matmul(dZ, self.weights.T)\n",
    "        \n",
    "        self.weights -= learning_rate * dW\n",
    "        self.biases -= learning_rate * db\n",
    "        \n",
    "        return dX\n",
    "    \n",
    "    def sigmoid(self, x):\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "    \n",
    "    def sigmoid_derivative(self, x):\n",
    "        return x * (1 - x)\n",
    "\n",
    "    def mse_loss(self, output, target):\n",
    "        return np.mean((output - target) ** 2)\n",
    "\n",
    "    def mse_loss_derivative(self, predicted, targets):\n",
    "        return predicted - targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 483,
   "id": "399f368c-9cf8-4dd8-80ca-7af73c486659",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 3\n",
    "output_size = 2\n",
    "# Create MLP with input size 10 and output size 5\n",
    "mlp = MLP(input_size, output_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 484,
   "id": "7acc0f76-0eab-46ed-bde8-a5b5e5abb87d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.63462094, 0.39361231],\n",
       "       [0.22967546, 0.24832513],\n",
       "       [0.42406899, 0.03676409],\n",
       "       [0.42979041, 0.01736074],\n",
       "       [0.83922517, 0.68523752],\n",
       "       [0.09887785, 0.99872105],\n",
       "       [0.9090212 , 0.96892209],\n",
       "       [0.09468154, 0.50754518],\n",
       "       [0.16805481, 0.83964839],\n",
       "       [0.14557361, 0.00283616]])"
      ]
     },
     "execution_count": 484,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = mlp.forward(X)\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 485,
   "id": "65a598a1-6dce-4e07-b016-5f244b7810f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ -0.67011504,   0.9306511 ,   1.28544575],\n",
       "       [  0.69752453,   3.07545823,  10.90657995],\n",
       "       [  3.54412038,  -8.25456994, -16.88841197],\n",
       "       [ -4.98914012,  12.8603824 ,  27.52927954],\n",
       "       [  2.11983318,  -1.52224996,   0.23831816],\n",
       "       [ 10.32569174,   7.28629802,  45.67176307],\n",
       "       [  0.29188637,   6.82904833,  21.34383405],\n",
       "       [  2.0996355 ,  -1.45020637,   0.41026106],\n",
       "       [ -5.33149132,   6.45319285,   7.347319  ],\n",
       "       [ -8.55133818,  11.95554139,  16.64428419]])"
      ]
     },
     "execution_count": 485,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dX = mlp.backward(Y, learning_rate)\n",
    "dX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 438,
   "id": "5bbba1bb-8606-48d5-86d3-164a200d893f",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp.weights = t_mlp.fc1.weight.detach().numpy().T\n",
    "mlp.biases = t_mlp.fc1.bias.detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "id": "88960928-107c-4bb1-adf2-9310dd102ed5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.2781462   0.4059974   0.42012388]\n",
      " [-0.4212348  -0.12950394  0.38296783]]\n",
      "[-0.23468414  0.10032433]\n"
     ]
    }
   ],
   "source": [
    "print(mlp.weights.T)\n",
    "print(mlp.biases.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "id": "0a7b529e-2824-4f3d-b007-356828536d72",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.44722921, 0.40476478],\n",
       "       [0.42615047, 0.2452808 ],\n",
       "       [0.68599789, 0.36058244],\n",
       "       [0.38489515, 0.43478932],\n",
       "       [0.47692172, 0.43691784],\n",
       "       [0.23973604, 0.25751712],\n",
       "       [0.4638022 , 0.27973996],\n",
       "       [0.50765635, 0.51952948],\n",
       "       [0.38347513, 0.58258832],\n",
       "       [0.60745182, 0.63928659]])"
      ]
     },
     "execution_count": 375,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = mlp.forward(X)\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "id": "dea65139-7c3b-4ebf-8f26-d83382bb301c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.4472, 0.4048],\n",
       "        [0.4262, 0.2453],\n",
       "        [0.6860, 0.3606],\n",
       "        [0.3849, 0.4348],\n",
       "        [0.4769, 0.4369],\n",
       "        [0.2397, 0.2575],\n",
       "        [0.4638, 0.2797],\n",
       "        [0.5077, 0.5195],\n",
       "        [0.3835, 0.5826],\n",
       "        [0.6075, 0.6393]], grad_fn=<SigmoidBackward0>)"
      ]
     },
     "execution_count": 376,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 4: Forward pass\n",
    "out = t_mlp(torch.tensor(X, dtype=torch.float32))\n",
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "id": "ee45a407-901e-45b5-8846-15ad7df36541",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.3167, grad_fn=<MseLossBackward0>)"
      ]
     },
     "execution_count": 377,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "# Step 2: Define the loss function\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# Step 3: Define the optimizer\n",
    "learning_rate = 0.1\n",
    "optimizer = optim.SGD(t_mlp.parameters(), lr=learning_rate)\n",
    "\n",
    "# Step 5: Compute the loss\n",
    "loss = criterion(out, torch.tensor(Y, dtype=torch.float32))\n",
    "loss.retain_grad()\n",
    "\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "id": "bb55f0e6-0236-4012-a427-df1d7933e03b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 6: Backpropagation\n",
    "optimizer.zero_grad()  # Clear gradients\n",
    "loss.backward()  # Compute gradients\n",
    "optimizer.step()  # Update model parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "id": "97c1fd03-3185-4b27-aa4b-6b3500c5c135",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.)"
      ]
     },
     "execution_count": 379,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "id": "dd979550-5174-476a-a1ba-5a274623562d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0092, -0.0114],\n",
       "        [ 0.1356,  0.0200],\n",
       "        [ 0.1152,  0.0557],\n",
       "        [ 0.0315,  0.1680],\n",
       "        [ 0.0461,  0.0894],\n",
       "        [-0.0040, -0.1300],\n",
       "        [-0.1246,  0.1879],\n",
       "        [ 0.2366,  0.1076],\n",
       "        [ 0.0277,  0.1024],\n",
       "        [ 0.1990,  0.0499]])"
      ]
     },
     "execution_count": 380,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_mlp.out.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "id": "6450b13e-cede-4caa-b2eb-5e24bedcde1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform backward pass\n",
    "learning_rate = 0.1\n",
    "dX = mlp.backward(Y, learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "id": "db5eccdb-5012-4043-b9d8-834570ea115a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A=(10, 2)\n",
      "X=(10, 3)\n",
      "dA=(10, 2)\n",
      "dX=(10, 3)\n",
      "W=(3, 2)\n",
      "b=(2,)\n",
      "z=(10, 2)\n"
     ]
    }
   ],
   "source": [
    "print(f\"A={output.shape}\")\n",
    "print(f\"X={X.shape}\")\n",
    "print(f\"dA={dA.shape}\")\n",
    "print(f\"dX={dX.shape}\")\n",
    "print(f\"W={mlp.weights.shape}\")\n",
    "print(f\"b={mlp.biases.shape}\")\n",
    "print(f\"z={mlp.z.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "id": "2c430ec3-b1f6-46cd-9296-e801cdb1a618",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.2866298   0.36066747  0.3000821 ]\n",
      " [-0.05485079  0.28255358  0.50762576]]\n",
      "[-0.26092786  0.2804039 ]\n"
     ]
    }
   ],
   "source": [
    "print(mlp.weights.T)\n",
    "print(mlp.biases.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "id": "9c46186e-d4f1-452b-9ebe-1fa7dca40ab9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[ 0.2866,  0.3607,  0.3001],\n",
      "        [-0.0549,  0.2826,  0.5076]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-0.2609,  0.2804], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "print(t_mlp.fc1.weight)\n",
    "print(t_mlp.fc1.bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "id": "05b59b40-5bd0-446f-ae22-09e35ebd1a15",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.66351810314361"
      ]
     },
     "execution_count": 254,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp.mse_loss(Y, mlp.a)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
